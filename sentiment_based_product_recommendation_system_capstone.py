# -*- coding: utf-8 -*-
"""Sentiment-Based Product Recommendation System-Capstone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p6-Z_Nx6KGNZ1aKeSRXT2fnTfA18tpmP

## Sentiment-Based Product Recommendation System - Capstone - Sagar Kulkarni

#### Problem Statement : Build a model that will improve the recommendations given to the users given their past reviews and ratings.

#### The Project implements the follwing tasks
1. Data sourcing and sentiment analysis
2. Building a recommendation system
3. Improving the recommendations using the sentiment analysis model
4. Deploying the end-to-end project with a user interface

## 1. Data sourcing and sentiment analysis
"""

# import necessary libraries

import pandas as pd
import numpy as np
from numpy import *
import os
import matplotlib.pyplot as plt
import seaborn as sns

#nltk
import nltk
#import scikitplot as skplt
import re
from nltk.corpus import stopwords
nltk.download('stopwords')
STOPWORDS = stopwords.words('english')

#model building libraries
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report
from sklearn import metrics
from imblearn.over_sampling import SMOTE
from sklearn.metrics.pairwise import pairwise_distances
from sklearn.model_selection import RandomizedSearchCV
import pickle

#suppress warnings
import warnings
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/drive/')

!pwd

os.chdir('/content/drive/My Drive/Upgrad/Sentiment Based Product Recommendation/')

df = pd.read_csv('sample30.csv')

#view top 5 the data
df.head(5)

# check the shape of data
df.shape

# check the columns and data types
df.info()

# check the nulls in the data per column

df.isnull().sum()

# based on above info and requirement of the project, identy and drop columns
# that are not required / useful for EDA and recommendations

df.drop(['reviews_date', 'manufacturer','reviews_didPurchase', 'reviews_doRecommend', 
              'reviews_userCity','reviews_userProvince'], axis=1, inplace=True)

# check again the nulls in the data per column

df.isnull().sum()

# remove all the null values and reset the index of the df
df = df.dropna()
df.reset_index(inplace=True, drop=True)

# check again the nulls in the data per column, all nulls have been handled

df.isnull().sum()

# check the statistics of all columns

df.describe()

# check for unique values per column
df.nunique()

#check the current shape of the df

df.shape

# check and drop all duplicate values

df.drop_duplicates(inplace=True)

#verify the new shape of the df

df.shape

#Retrieve products with most number of ratings

Products_by_Count = pd.DataFrame(df.groupby('id')['reviews_rating'].count())
Products_by_Count = Products_by_Count.sort_values('reviews_rating', ascending=False)
Products_by_Count.head(10)

#plot the top 10 products by count

Products_by_Count.head(10).plot(kind = "bar")

# mean ratings of each product

top_rated_product = pd.DataFrame(df.groupby('id')['reviews_rating'].mean())
top_rated = top_rated_product.sort_values('reviews_rating', ascending=False)
top_rated.head(10)

# plotting a histogram for top rated products
top_rated.plot(kind = "hist")

# retrieve all the least reviewed products.
least_reviewed_products = Products_by_Count.query('reviews_rating < 5')

# check the number of records with least reviewed products
least_reviewed_products.shape

# remove these entries from the original data set

df = df[~df.id.isin(list(least_reviewed_products.index))]

#re-checking mean rating for each product
top_rated = pd.DataFrame(df.groupby('id')['reviews_rating'].mean())
top_rated = top_rated.sort_values('reviews_rating', ascending=False)
top_rated.head(10)

# modifying user sentiment column values

df['user_sentiment'][df['user_sentiment'] == 'Positive'] = 1
df['user_sentiment'][df['user_sentiment'] == 'Negative'] = 0

df[['user_sentiment']].value_counts()

df[['reviews_title']].value_counts()

#merging title and text column to get model features
df['review'] = df['reviews_title']+' '+df['reviews_text']

# creating tfidf vector for review title and text
tfidf = TfidfVectorizer()

#cleaning text, lowercasing, special character removal, extra space removal and stopwords removal
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^0-9a-zA-Z]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    text = " ".join(word for word in text.split() if word not in STOPWORDS)
    return text

#Entry before cleaining Text

print(df.loc[16])

# Execute clean_text funtion

for i in range(len(df)):
    df['review'].iloc[i]= clean_text(df['review'].iloc[i])

# Check same record after cleaning
print(df.loc[16])

!pwd

# saving df to csv
#Dir = '/content/drive/My Drive/Upgrad/Sentiment Based Product Recommendation/Data/processed_data.csv'
#df.to_csv(Dir)

df.to_csv('./Data/CleanData.csv',index=False)

# joining review title and text for creating model features
X = df['review']
y = df['user_sentiment']
y=y.astype('int')

X.head()

# creating tfidf vector of review
X = tfidf.fit_transform(X)

#train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

# checking input shape
X_train.shape

print(X_train[0])

"""### Logistic Regression Classifier"""

#training logistic regression model
clf = LogisticRegression(random_state=0).fit(X_train, y_train)

#train accuracy
clf.score(X_train, y_train)

#test accuracy
clf.score(X_test, y_test)

#test predictions
predictions = clf.predict(X_test)

#logistic regression report
print(classification_report(y_test, predictions))

"""### Gradient Boosting Classifier"""

#training gradient boosting model
xgb = GradientBoostingClassifier(n_estimators=25, learning_rate=0.9, max_depth=6, random_state=0).fit(X_train, y_train)

#test accuracy
xgb.score(X_test, y_test)

#predictions on test data
predictions_xgb = xgb.predict(X_test)

#Gradient boosting model report
print(classification_report(y_test, predictions_xgb))

"""### We are not getting good results due to class imbalance issue. so let's handle it first.

## Class imbalance handling using SMOTE
"""

y.value_counts()

smote = SMOTE()
# fit predictor and target variable
X_smote, y_smote = smote.fit_resample(X, y)

print('Resample dataset shape:\n', pd.Series(y_smote).value_counts())

#Spliting train and test dataset 
X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.30, random_state=42)

"""## Logistic regression with SMOTE"""

#training logistic regression on new data
clf = LogisticRegression(random_state=0, solver='liblinear').fit(X_train, y_train)

#train accuracy
clf.score(X_train, y_train)

#test accuracy
clf.score(X_test, y_test)

#predictions on test
predictions = clf.predict(X_test)

# logistic regression model report after handling class imbalance
print(classification_report(y_test, predictions))

"""## Gradient Boosting with SMOTE"""

#training gradient boosting model
xgb = GradientBoostingClassifier(n_estimators=25, learning_rate=0.9, max_depth=6, random_state=0).fit(X_train, y_train)

# test accuracy
xgb.score(X_test, y_test)

#predictions on test
predictions_xgb = xgb.predict(X_test)

# gradient boosting model performance after handling class imbalance
print(classification_report(y_test, predictions_xgb))

"""## Random forest classifier with SMOTE"""

# training random forest model
rfc = RandomForestClassifier(n_estimators=50, max_depth=6, random_state=0).fit(X_train, y_train)

# test accuracy
rfc.score(X_test, y_test)

#predictions on test model
predictions_rfc = rfc.predict(X_test)

# random forest model performance after handling class imbalance
print(classification_report(y_test, predictions_rfc))

"""### In these 3, logistic regression is giving best results. Let's try hyper parameter tunning now.

# Hyper Parameter Tuning

### logistic regression with HPT
"""

# define models and parameters

solvers = ['newton-cg', 'lbfgs', 'liblinear']
penalty = ['l1','l2']
c_values = [10, 1.0, 0.1, 0.01]

# Create the random grid
random_grid = {'solver': solvers,
               'penalty': penalty,
               'C': c_values}
print(random_grid)

"""#Use the random grid to search for best hyperparameters
#First create the base model to tune
lr_hpt = LogisticRegression()
#Random search of parameters, using 3 fold cross validation, 
#search across 100 different combinations, and use all available cores
lr_random = RandomizedSearchCV(estimator = lr_hpt, param_distributions = random_grid, n_iter = 20, cv = 3,
                               verbose=2, random_state=42, n_jobs = -1)
#Fit the random search model
lr_random.fit(X_train, y_train)

#best logistic regression model
lr_random = lr_random.best_params_
"""

#best model after HPT
lr_random = LogisticRegression(solver='liblinear', penalty= 'l1',C= 10, verbose=2, 
                               random_state=42, n_jobs = -1).fit(X_train, y_train)

#predictions on test data
predictions_lr = lr_random.predict(X_test)

# report of best logistic regression model
print(classification_report(y_test, predictions_lr))

"""### Random Forest"""

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]

# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
print(random_grid)

"""#Use the random grid to search for best hyperparameters
#First create the base model to tune
rfc_hpt = RandomForestClassifier()
#Random search of parameters, using 3 fold cross validation, 
#search across 100 different combinations, and use all available cores
rf_random = RandomizedSearchCV(estimator = rfc_hpt, param_distributions = random_grid, n_iter = 100, 
                               cv = 3, verbose=2, random_state=42, n_jobs = -1)
#Fit the random search model
rf_random.fit(X_train, y_train)

#best random forest model
rf_random.best_params_
"""

#saving best model to save time in multiple runs
rf_random = RandomForestClassifier(n_estimators = 73, min_samples_split= 5, min_samples_leaf = 1, max_features  = 'sqrt',
 max_depth= 100, bootstrap= False, random_state=42, n_jobs = -1).fit(X_train, y_train)

# accuracy on test data
rf_random.score(X_test, y_test)

# predictions on test data
predictions_rfc = rf_random.predict(X_test)

# report of best random forest model
print(classification_report(y_test, predictions_rfc))

"""### XGBoost"""

# Number of trees in xgboost
n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# LR of levels in tree
learning_rate = [(x) for x in np.linspace(0.1,1, num = 5)]
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]

# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
              'learning_rate': learning_rate}
print(random_grid)

"""#Use the random grid to search for best hyperparameters
#First create the base model to tune
xgb_hpt = GradientBoostingClassifier()
#Random search of parameters, using 3 fold cross validation, 
#search across 100 different combinations, and use all available cores
xgb_random = RandomizedSearchCV(estimator = xgb_hpt, param_distributions = random_grid, 
                                n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
#Fit the random search model
xgb_random.fit(X_train, y_train)

#best xgboost model
xgb_random.best_params_
"""

xgb_random =GradientBoostingClassifier(learning_rate =0.325, max_depth = None, max_features = 'sqrt',min_samples_split=2,
                                       n_estimators=94,random_state=42).fit(X_train, y_train)

#test accuracy
xgb_random.score(X_test, y_test)

#predictions on test data
predictions_xgb = xgb_random.predict(X_test)

#best xgboost model report
print(classification_report(y_test, predictions_xgb))

"""## Model Comparison"""

start = "\033[1m"
end = "\033[0;0m"

print(start+'classification report of Final Logistic regression Model:\n'+end)
print(classification_report(y_test, predictions_lr))
print('*'*80+'\n')

print(start+'classification report of Final Random Forest Model:\n'+end)
print(classification_report(y_test, predictions_rfc))
print('*'*80+'\n')

print(start+'classification report of Final XGBoost Model:\n'+end)
print(classification_report(y_test, predictions_xgb))
print('*'*80+'\n')

"""### XGBoost model is giving best results, we choose it as final model."""

if not os.path.exists('Models'):
    os.makedirs('Models')

#Dir2 = '/content/drive/My Drive/Upgrad/Sentiment Based Product Recommendation/Models/sentiment_detection.pkl'
#filename = 'sentiment_detection.pkl'
#pickle.dump(xgb_random, open(Dir2, 'wb'))

filename = 'Sentiment_Analysis.pkl'
pickle.dump(xgb_random, open('./Models/'+filename, 'wb'))

# load the model from disk
loaded_model = pickle.load(open('./Models/Sentiment_Analysis.pkl', 'rb'))
result = loaded_model.score(X_test, y_test)
result

loaded_model.predict(X_test)

"""# Collaborative filtering

## Dividing the dataset into train and test
"""

df = df.copy()

# Test and Train split of the dataset.

train, test = train_test_split(df, test_size=0.30, random_state=31)

print(train.shape)
print(test.shape)

# Pivot the train ratings' dataset into matrix format in which columns are products and the rows are user IDs.
df_pivot = train.pivot_table(values='reviews_rating', index='reviews_username', columns='id', fill_value=0)

df_pivot.head()

#checking pivot table for entries
df_pivot.query('reviews_username == "00dog3"')["AVpe6FfKilAPnD_xQmHi"]

#checking pivot table for entries
df_pivot.query('reviews_username == "jds1992"')["AV16khLE-jtxr-f38VFn"]

"""### Creating dummy train & dummy test dataset
These dataset will be used for prediction 
- Dummy train will be used later for prediction of the products which has not been rated by the user. To ignore the products rated by the user, we will mark it as 0 during prediction. The products not rated by user is marked as 1 for prediction in dummy train dataset. 

- Dummy test will be used for evaluation. To evaluate, we will only make prediction on the products rated by the user. So, this is marked as 1. This is just opposite of dummy_train.
"""

# Copy the train dataset into dummy_train
dummy_train = train.copy()

# The products not rated by user is marked as 1 for prediction. 
dummy_train['reviews_rating'] = dummy_train['reviews_rating'].apply(lambda x: 0 if x>=1 else 1)

# Convert the dummy train dataset into matrix format.
dummy_train = dummy_train.pivot_table(values='reviews_rating', index='reviews_username', columns='id', fill_value=1)
dummy_train.head()

"""## Using adjusted Cosine

### Here, we are not removing the NaN values and calculating the mean only for the products rated by the user
"""

# Create a user-product matrix.
df_pivot = train.pivot_table(
    index='reviews_username',
    columns='id',
    values='reviews_rating', aggfunc='mean'
)

df_pivot.head()

df_pivot.shape

"""### Normalising the rating of the product for each user around 0 mean"""

mean = np.nanmean(df_pivot, axis=1)

mean.shape

#subtract mean
df_subtracted = (df_pivot.T-mean).T

df_subtracted.head()

"""### Finding cosine similarity"""

# Creating the User Similarity Matrix using pairwise_distance function.
user_correlation = 1 - pairwise_distances(df_subtracted.fillna(0), metric='cosine')
user_correlation[np.isnan(user_correlation)] = 0
print(user_correlation)

"""## Prediction - User User

Doing the prediction for the users which are positively related with other users, and not the users which are negatively related as we are interested in the users which are more similar to the current users. So, ignoring the correlation for values less than 0.
"""

user_correlation[user_correlation<0]=0
user_correlation

user_predicted_ratings = np.dot(user_correlation, df_pivot.fillna(0))
user_predicted_ratings

user_predicted_ratings.shape

"""Since we are interested only in the products not rated by the user, we will ignore the products rated by the user by making it zero. """

user_final_rating = np.multiply(user_predicted_ratings,dummy_train)
user_final_rating.head()

"""### Finding the top 5 recommendation for the *user*"""

# Take the user ID as input.
#user_input = (input("Enter your user name"))
user_input='00dog3'

d = user_final_rating.loc[user_input].sort_values(ascending=False)[0:5]
d

"""# Evaluation - User User

Evaluation will we same as you have seen above for the prediction. The only difference being, you will evaluate for the product already rated by the user insead of predicting it for the product not rated by the user.
"""

test.shape

# Find out the common users of test and train dataset.
common = test[test.reviews_username.isin(train.reviews_username)]
common.shape

common.head()

# convert into the user-product matrix.
common_user_based_matrix = common.pivot_table(index='reviews_username', columns='id', values='reviews_rating')

common_user_based_matrix.head()

# Convert the user_correlation matrix into dataframe.
user_correlation_df = pd.DataFrame(user_correlation)

df_subtracted.head()

df_subtracted.index

user_correlation_df['reviews_username'] = df_subtracted.index
user_correlation_df.set_index('reviews_username',inplace=True)
user_correlation_df.head()

common.head()

list_name = common.reviews_username.tolist()

user_correlation_df.columns = df_subtracted.index.tolist()


user_correlation_df_1 =  user_correlation_df[user_correlation_df.index.isin(list_name)]

user_correlation_df_1.shape

user_correlation_df_2 = user_correlation_df_1.T[user_correlation_df_1.T.index.isin(list_name)]

user_correlation_df_3 = user_correlation_df_2.T

user_correlation_df_3.head()

user_correlation_df_3.shape

user_correlation_df_3[user_correlation_df_3<0]=0

common_user_predicted_ratings = np.dot(user_correlation_df_3, common_user_based_matrix.fillna(0))
common_user_predicted_ratings

dummy_test = common.copy()

dummy_test['reviews_rating'] = dummy_test['reviews_rating'].apply(lambda x: 1 if x>=1 else 0)

dummy_test = dummy_test.pivot_table(index='reviews_username', columns='id', values='reviews_rating').fillna(0)

dummy_test.shape

common_user_predicted_ratings = np.multiply(common_user_predicted_ratings,dummy_test)

common_user_predicted_ratings.head(5)

"""Calculating the RMSE for only the products rated by user. For RMSE, normalising the rating to (1,5) range."""

X  = common_user_predicted_ratings.copy() 
X = X[X>0]

scaler = MinMaxScaler(feature_range=(1, 5))
print(scaler.fit(X))
y = (scaler.transform(X))

print(y)

common_ = common.pivot_table(index='reviews_username', columns='id', values='reviews_rating')

# Finding total non-NaN value
total_non_nan = np.count_nonzero(~np.isnan(y))

rmse = (sum(sum((common_ - y )**2))/total_non_nan)**0.5
print(rmse)

"""## Using Item similarity

# Item Based Similarity

Taking the transpose of the rating matrix to normalize the rating around the mean for different  product ID. In the user based similarity, we had taken mean for each user instead of each product.
"""

df_pivot = train.pivot_table(
    index='reviews_username',
    columns='id',
    values='reviews_rating', aggfunc='mean'
).T

df_pivot.head()

"""Normalising the product rating for each product for using the Adujsted Cosine"""

mean = np.nanmean(df_pivot, axis=1)

df_subtracted = (df_pivot.T-mean).T

df_subtracted.head(5)

"""Finding the cosine similarity using pairwise distances approach"""

# Item Similarity Matrix
item_correlation = 1 - pairwise_distances(df_subtracted.fillna(0), metric='cosine')
item_correlation[np.isnan(item_correlation)] = 0
print(item_correlation)

"""Filtering the correlation only for which the value is greater than 0. (Positively correlated)"""

item_correlation[item_correlation<0]=0
item_correlation

"""# Prediction - Item Item"""

item_predicted_ratings = np.dot((df_pivot.fillna(0).T),item_correlation)
item_predicted_ratings

item_predicted_ratings.shape

dummy_train.shape

"""### Filtering the rating only for the products not rated by the user for recommendation"""

item_final_rating = np.multiply(item_predicted_ratings,dummy_train)
item_final_rating.head(5)

"""### Finding the top 20 recommendation for the *user*"""

item_final_rating.loc['0325home']

recomm_20 = item_final_rating.loc['0325home'].sort_values(ascending=False)[0:20]

recomm_20

"""# Evaluation - Item Item

Evaluation will we same as you have seen above for the prediction. The only difference being, you will evaluate for the product already rated by the user insead of predicting it for the product not rated by the user.
"""

test.columns

common =  test[test.id.isin(train.id)]
common.shape

common.head(4)

common_item_based_matrix = common.pivot_table(index='reviews_username', columns='id', values='reviews_rating').T

common_item_based_matrix.shape

item_correlation_df = pd.DataFrame(item_correlation)

item_correlation_df.head(2)

item_correlation_df['id'] = df_subtracted.index
item_correlation_df.set_index('id',inplace=True)
item_correlation_df.head(5)

list_name = common.id.tolist()
item_correlation_df.columns = df_subtracted.index.tolist()

item_correlation_df_1 =  item_correlation_df[item_correlation_df.index.isin(list_name)]
item_correlation_df_2 = item_correlation_df_1.T[item_correlation_df_1.T.index.isin(list_name)]

item_correlation_df_3 = item_correlation_df_2.T

item_correlation_df_3.head(5)

item_correlation_df_3[item_correlation_df_3<0]=0

common_item_predicted_ratings = np.dot(item_correlation_df_3, common_item_based_matrix.fillna(0))
common_item_predicted_ratings

common_item_predicted_ratings.shape

"""Dummy test will be used for evaluation. To evaluate, we will only make prediction on the products rated by the user. So, this is marked as 1. This is just opposite of dummy_train"""

dummy_test = common.copy()

dummy_test['reviews_rating'] = dummy_test['reviews_rating'].apply(lambda x: 1 if x>=1 else 0)

dummy_test = dummy_test.pivot_table(index='reviews_username', columns='id', values='reviews_rating').T.fillna(0)

common_item_predicted_ratings = np.multiply(common_item_predicted_ratings,dummy_test)

"""The products not rated is marked as 0 for evaluation. And make the item- item matrix representaion."""

common_ = common.pivot_table(index='reviews_username', columns='id', values='reviews_rating').T

X  = common_item_predicted_ratings.copy() 
X = X[X>0]

scaler = MinMaxScaler(feature_range=(1, 5))
print(scaler.fit(X))
y = (scaler.transform(X))

print(y)

# Finding total non-NaN value
total_non_nan = np.count_nonzero(~np.isnan(y))

rmse = (sum(sum((common_ - y )**2))/total_non_nan)**0.5
print(rmse)

"""# We choose Item based collaborative filtering as in user based collaborative filtering we are getting similarity as zero due to less reviews by users."""

# using full data to get item recommendation for all users

df_pivot = df.pivot_table(
    index='reviews_username',
    columns='id',
    values='reviews_rating', aggfunc='mean'
).T

df_pivot.head()

"""Normalising the product rating for each product for using the Adujsted Cosine"""

mean = np.nanmean(df_pivot, axis=1)

df_subtracted = (df_pivot.T-mean).T

"""Finding the cosine similarity using pairwise distances approach"""

# Item Similarity Matrix
item_correlation = 1 - pairwise_distances(df_subtracted.fillna(0), metric='cosine')
item_correlation[np.isnan(item_correlation)] = 0
print(item_correlation)

"""Filtering the correlation only for which the value is greater than 0. (Positively correlated)"""

item_correlation[item_correlation<0]=0
item_correlation

#user item matrix
item_predicted_ratings = np.dot((df_pivot.fillna(0).T),item_correlation)
item_predicted_ratings

item_predicted_ratings.shape

# Copy the df dataset into dummy_reviews
dummy_df = df.copy()

# The products not rated by user is marked as 1 for prediction. 
dummy_df['reviews_rating'] = dummy_df['reviews_rating'].apply(lambda x: 0 if x>=1 else 1)

# Convert the dummy df dataset into matrix format.
dummy_df = dummy_df.pivot_table(values='reviews_rating', index='reviews_username', columns='id', fill_value=1)
dummy_df.head()

dummy_df.shape

"""### Filtering the rating only for the products not rated by the user for recommendation"""

item_final_rating = np.multiply(item_predicted_ratings,dummy_df)
item_final_rating.head()

# saving dataframe to csv for later use in main file
#Dir1 = '/content/drive/My Drive/Upgrad/Sentiment Based Product Recommendation/Data/item_final_rating.csv'
#item_final_rating.to_csv(Dir1)

item_final_rating.to_csv('./Data/Sentiment_Analysis_ItemBased_Final_Ratings.csv')

recomm_20 = item_final_rating.loc[user_input].sort_values(ascending=False)[0:20]
recomm_20

# getting mean sentiment for each user
def id_to_mean(product_id):
    df_id = df.loc[df['id']==product_id]
    X_id = tfidf.transform(df_id['review'])
    output = loaded_model.predict(X_id)
    return np.mean(output)

# product and product sentiment
final_df= dict()
for prod in recomm_20.index:
    final_df[prod]=id_to_mean(prod)

final_df

# top 5 products by user sentiment
final_df = dict(sorted(final_df.items(), key = lambda kv:(kv[1], kv[0]), reverse=True)[:5])
final_df

# product id and name pairs for top 5 products
for key in final_df.keys():
    value = df[df['id']==key]['name'].drop_duplicates().iloc[0]
    final_df[key]=value

final_df

# top 5 products as dataframe
output = pd.DataFrame(list(final_df.items()), columns=['Id', 'Name'])

output

